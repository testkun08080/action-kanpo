#!/usr/bin/env python3
"""
fetch_kanpo.py
"""

import os
from pathlib import Path

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
from urllib.parse import urljoin, urlparse
import re


class KanpoFetcher:
    def __init__(self, test_mode=True):
        self.base_url = "https://www.kanpo.go.jp"
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        self.test_mode = test_mode

    def get_today_date(self):
        """ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—"""
        return datetime.now().strftime("%Y-%m-%d")

    def create_date_folder(self, date_str):
        """æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ"""
        base_folder = Path(__file__).resolve().parent.parent / "kanpo"

        folder_path = base_folder / date_str
        folder_path.mkdir(parents=True, exist_ok=True)
        return folder_path

    def fetch_main_page(self):
        """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        try:
            print("ğŸŒ å®˜å ±ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
            response = self.session.get(self.base_url + "/index.html", timeout=10)
            response.raise_for_status()
            print("âœ… ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ")
            return BeautifulSoup(response.content, "html.parser")
        except Exception as e:
            print(f"âŒ ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {e}")
            return None

    def find_today_kanpo_link(self, soup, class_filter=None):
        """ä»Šæ—¥ã®å®˜å ±ã®ãƒªãƒ³ã‚¯ã‚’ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‹ã‚‰æ¢ã™"""
        print("ğŸ” ä»Šæ—¥ã®å®˜å ±ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ä¸­...")
        today = datetime.now()

        # æ¢ç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©
        patterns = [
            today.strftime("%Y%m%d"),
        ]

        print(f"ğŸ“… æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: {patterns}")
        print(f"ğŸ¯ ä½¿ç”¨ã™ã‚‹ã‚¯ãƒ©ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {class_filter}")

        links = []

        # ã‚¯ãƒ©ã‚¹æŒ‡å®šãŒã‚ã‚‹å ´åˆã¯ãã‚Œã«çµã£ã¦æ¢ã™
        if class_filter:
            link_candidates = soup.find_all("a", href=True, class_=class_filter)
        else:
            link_candidates = soup.find_all("a", href=True)

        for link in link_candidates:
            link_text = link.get_text(strip=True)
            href = link.get("href", "")

            for pattern in patterns:
                if pattern in link_text or pattern in href:
                    full_url = urljoin(self.base_url, href)
                    links.append({"url": full_url, "text": link_text, "pattern_matched": pattern})
                    print(f"  âœ… ç™ºè¦‹: {link_text} (ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern})")
                    break

        if not links:
            print("âš ï¸  ä»Šæ—¥ã®å®˜å ±ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print("ğŸ’¡ åˆ©ç”¨å¯èƒ½ãªãƒªãƒ³ã‚¯ã®ä¾‹:")
            for i, link in enumerate(link_candidates[:10]):
                text = link.get_text(strip=True)
                if text and len(text) > 3:
                    print(f"    {i + 1}. {text}")

        return links

    def fetch_kanpo_page(self, kanpo_url):
        """å®˜å ±ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦PDFãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆ<a> ã¨ <iframe> ã«å¯¾å¿œï¼‰"""
        print(f"ğŸ“– å®˜å ±ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ä¸­: {kanpo_url}")
        try:
            response = self.session.get(kanpo_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            pdf_links = []

            # <div class="pdfHeader"> å†…ã® class="date" è¦ç´ ã‚’å–å¾—
            date_text = None
            pdf_header = soup.find("div", class_="pdfHeader")
            if pdf_header:
                date_element = pdf_header.find(class_="date")
                if date_element:
                    date_text = date_element.get_text(strip=True)
                    print(f"ğŸ“… å®˜å ±ã®æ—¥ä»˜: {date_text}")
                else:
                    print("âš ï¸ pdfHeader å†…ã« date ã‚¯ãƒ©ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                print("âš ï¸ pdfHeader è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

            # --- <iframe src="...pdf"> ã®å‡¦ç† ---
            for iframe in soup.find_all("iframe", src=True):
                src = iframe["src"]
                if src.lower().endswith(".pdf"):
                    pdf_url = urljoin(kanpo_url, src)
                    pdf_name = src.split("/")[-1]  # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
                    pdf_links.append({
                        "url": pdf_url,
                        "name": date_text,
                        "filename": os.path.basename(urlparse(src).path),
                        "source": "iframe",
                    })
                    print(f"  ğŸ“„ PDFç™ºè¦‹ (<iframe>): {pdf_name}")

            print(f"ğŸ“Š ã“ã®ãƒšãƒ¼ã‚¸ã§ {len(pdf_links)} å€‹ã®PDFã‚’ç™ºè¦‹")
            return pdf_links

        except Exception as e:
            print(f"âŒ å®˜å ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•— {kanpo_url}: {e}")
            return []

    def download_pdf(self, pdf_info, folder_path):
        """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {pdf_info['name']}")

            if self.test_mode:
                # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã®ã¿å–å¾—
                response = self.session.head(pdf_info["url"], timeout=10)
                response.raise_for_status()

                content_length = response.headers.get("Content-Length", "ä¸æ˜")
                content_type = response.headers.get("Content-Type", "ä¸æ˜")

                print("  âœ… ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªæˆåŠŸ")
                print(f"  ğŸ“Š ã‚µã‚¤ã‚º: {content_length} bytes")
                print(f"  ğŸ“‹ ã‚¿ã‚¤ãƒ—: {content_type}")

                # ãƒ†ã‚¹ãƒˆç”¨ã«ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                safe_filename = re.sub(r"[^\w\-_\.]", "_", pdf_info["filename"])
                if not safe_filename.endswith(".pdf"):
                    safe_filename += ".pdf"

                file_path = folder_path / safe_filename
                file_path.write_text(f"ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« - {pdf_info['name']}\nä½œæˆæ—¥æ™‚: {datetime.now()}")

                return True
            else:
                # å®Ÿéš›ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                response = self.session.get(pdf_info["url"], stream=True, timeout=30)
                response.raise_for_status()

                safe_filename = re.sub(r"[^\w\-_\.]", "_", pdf_info["filename"])
                if not safe_filename.endswith(".pdf"):
                    safe_filename += ".pdf"

                file_path = folder_path / safe_filename

                with open(file_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                print(f"  âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {safe_filename}")
                return True

        except Exception as e:
            print(f"  âŒ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•— {pdf_info['url']}: {e}")
            return False

    def create_readme(self, folder_path, pdf_list, date_str):
        """READMEãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        file_name = f"å®˜å ±_{date_str}.md"
        readme_path = os.path.join(folder_path, file_name)

        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(f"# å®˜å ± {date_str}\n\n")
            f.write(f"å–å¾—æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if self.test_mode else 'ç„¡åŠ¹'}\n\n")
            f.write("## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«\n\n")

            for pdf_info in pdf_list:
                f.write(f"- [{pdf_info['name']}]({pdf_info['filename']})\n")
                f.write(f"  - URL: {pdf_info['url']}\n")

        print(f"ğŸ“{readme_path} ã‚’ä½œæˆ")

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        print("ğŸš€ å®˜å ±è‡ªå‹•å–å¾—ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹...")
        print("=" * 60)
        all_pdfs = []
        found_today = False

        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—
        soup = self.fetch_main_page()
        if not soup:
            print("âŒ ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return found_today

        # ä»Šæ—¥ã®å®˜å ±ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        today_links = self.find_today_kanpo_link(soup, "pdfDlb")
        if not today_links:
            print("âŒ ä»Šæ—¥ã®å®˜å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return found_today, all_pdfs

        print("ğŸ“… ä»Šæ—¥ã®å®˜å ±ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
        print(f"\nğŸ“‹ å‡¦ç†å¯¾è±¡: {len(today_links)} å€‹ã®å®˜å ±ãƒªãƒ³ã‚¯")
        found_today = True

        # æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        today_str = self.get_today_date()
        folder_path = self.create_date_folder(today_str)
        print(f"ğŸ“ ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€: {folder_path}")

        # å„å®˜å ±ãƒšãƒ¼ã‚¸ã‹ã‚‰PDFã‚’å–å¾—
        for i, link_info in enumerate(today_links):
            print(f"\n--- å®˜å ±ãƒšãƒ¼ã‚¸ {i + 1}/{len(today_links)} ---")
            print(f"ğŸ“– å‡¦ç†ä¸­: {link_info['text']}")

            pdf_links = self.fetch_kanpo_page(link_info["url"])

            for pdf_info in pdf_links:
                if self.download_pdf(pdf_info, folder_path):
                    all_pdfs.append(pdf_info)
                time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›

        # çµæœã®ã¾ã¨ã‚
        print("\n" + "=" * 60)
        print("ğŸ“Š å®Ÿè¡Œçµæœ")
        print("=" * 60)

        if all_pdfs:
            self.create_readme(folder_path, all_pdfs, today_str)
            print(f"âœ… æˆåŠŸ: {len(all_pdfs)} å€‹ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¾ã—ãŸ")
            print(f"ğŸ“ ä¿å­˜å ´æ‰€: {folder_path}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
            print("\nğŸ“„ å–å¾—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«:")
            for i, pdf in enumerate(all_pdfs, 1):
                print(f"  {i}. {pdf['name']}")

            return found_today, all_pdfs
        else:
            print("âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return found_today, all_pdfs


def main(test_mode=True):
    print("å®˜å ±å–å¾—ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    fetcher = KanpoFetcher(test_mode=test_mode)
    found, pdfs = fetcher.run()

    print("\n" + "=" * 60)
    if found:
        print("ğŸ‰ å®˜å ±å–å¾—æˆåŠŸï¼")
        if test_mode:
            print("ğŸ’¡ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã®æˆåŠŸ")
        else:
            print("âœ… å®Ÿéš›ã®PDFå–å¾—æˆåŠŸ")
    else:
        print("âš ï¸ å®˜å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="å®˜å ±PDFè‡ªå‹•å–å¾—ãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--test", action="store_true", help="ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆPDFã‚’å®Ÿéš›ã«ä¿å­˜ã—ãªã„ï¼‰")
    args = parser.parse_args()

    try:
        main(test_mode=args.test)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")

        import traceback

        traceback.print_exc()
