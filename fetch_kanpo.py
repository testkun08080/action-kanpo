"""
å®˜å ±ã®ãƒšãƒ¼ã‚¸ã‚’è¦‹ã«è¡Œãã€å½“æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚
GitHub Actionã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã€çµæœã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
"""

import os
import re
import time
import argparse
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup


class KanpoFetcher:
    def __init__(self, test_mode=True, serch_date=None):
        """å®˜å ±ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã€PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¯ãƒ©ã‚¹
        Args:
            test_mode (bool, optional): _description_. Defaults to True.
            serch_date (datetime, optional): _description_. Defaults to None.
        """
        self.base_url = "https://www.kanpo.go.jp"
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "Mozilla/5.0"})
        self.test_mode = test_mode
        self.serch_date = serch_date or datetime.now()

    def get_serch_date(self):
        """æ—¥ä»˜æ–‡å­—åˆ—(YYYY-MM-DD)ã‚’å–å¾—

        Returns:
            str: æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰
        """

        return self.serch_date.strftime("%Y-%m-%d")

    def create_date_folder(self, date_str):
        """æ—¥ä»˜ã«åŸºã¥ã„ã¦ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã™

        Args:
            date_str (str): æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰

        Returns:
            str: ä½œæˆã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
        """
        base_folder = Path("kanpo")
        folder_path = base_folder / date_str
        folder_path.mkdir(parents=True, exist_ok=True)
        return folder_path

    def fetch_main_page(self):
        """å®˜å ±ã®ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¾ã™

        Returns:
            BeautifulSoup: ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        try:
            print("ğŸŒ å®˜å ±ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
            response = self.session.get(self.base_url + "/index.html", timeout=10)
            response.raise_for_status()
            print("âœ… ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ")
            return BeautifulSoup(response.content, "html.parser")
        except Exception as e:
            print(f"âŒ ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {e}")
            return None

    def find_kanpo_link(self, soup, class_filter=None):
        """å®˜å ±ã®ç™ºè¡Œãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ã—ã¾ã™

        Args:
            soup (BeautifulSoup): BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            class_filter (str, optional): æ¤œç´¢ç”¨ã®ã‚¯ãƒ©ã‚¹åãƒ•ã‚£ãƒ«ã‚¿. Defaults to None.

        Returns:
            _type_: _description_
        """

        print("ğŸ” å®˜å ±ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ä¸­...")

        target_date = self.serch_date
        patterns = [target_date.strftime("%Y%m%d")]
        print(f"ğŸ“… æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: {patterns}")
        print(f"ğŸ¯ ä½¿ç”¨ã™ã‚‹ã‚¯ãƒ©ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {class_filter}")

        links = []
        link_candidates = (
            soup.find_all("a", href=True, class_=class_filter) if class_filter else soup.find_all("a", href=True)
        )

        for link in link_candidates:
            link_text = link.get_text(strip=True)
            href = link.get("href", "")
            for pattern in patterns:
                if pattern in link_text or pattern in href:
                    full_url = urljoin(self.base_url, href)
                    links.append({"url": full_url, "text": link_text, "pattern_matched": pattern})
                    print(f"  âœ… ç™ºè¦‹: {link_text} (ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern})")
                    break

        if not links:
            print("âš ï¸ å®˜å ±ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            for i, link in enumerate(link_candidates[:10]):
                text = link.get_text(strip=True)
                if text and len(text) > 3:
                    print(f"    {i + 1}. {text}")
        return links

    def fetch_kanpo_page(self, kanpo_url):
        """å®˜å ±ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã€PDFãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã™

        Args:
            kanpo_url (str): å®˜å ±ã®URL

        Returns:
            list: å®˜å ±ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã—ãŸPDFãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """

        print(f"ğŸ“– å®˜å ±ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ä¸­: {kanpo_url}")

        try:
            response = self.session.get(kanpo_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            pdf_links = []
            date_text = None

            date_element = soup.find("p", class_="date")
            if date_element:
                date_text = date_element.get_text(strip=True)
                print(f"ğŸ“… å®˜å ±ã®æ—¥ä»˜: {date_text}")
            else:
                print("âš ï¸ p ã« date ã‚¯ãƒ©ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“")

            for iframe in soup.find_all("iframe", src=True):
                src = iframe["src"]
                if src.lower().endswith(".pdf"):
                    pdf_url = urljoin(kanpo_url, src)
                    pdf_name = os.path.basename(urlparse(src).path)
                    pdf_links.append({"url": pdf_url, "name": date_text, "filename": pdf_name, "source": "iframe"})
                    print(f"  ğŸ“„ PDFç™ºè¦‹ (<iframe>): {pdf_name}")

            print(f"ğŸ“Š PDFæ•°: {len(pdf_links)}")
            return pdf_links
        except Exception as e:
            print(f"âŒ å®˜å ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {e}")
            return []

    def download_pdf(self, pdf_info, folder_path):
        """å®˜å ±ã®PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™

        Args:
            pdf_info (dict): pdfã®æƒ…å ±ï¼ˆURLã€åå‰ã€ãƒ•ã‚¡ã‚¤ãƒ«åãªã©ï¼‰
            folder_path (str): ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹

        Returns:
            bool: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹
        """

        try:
            print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {pdf_info['name']}")
            if self.test_mode:
                response = self.session.head(pdf_info["url"], timeout=10)
                response.raise_for_status()
                print("  âœ… ç¢ºèªæˆåŠŸ")
                file_path = folder_path / (re.sub(r"[^\w\-_\.]", "_", pdf_info["filename"]) + ".pdf")
                file_path.write_text(f"ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« - {pdf_info['name']}\n")
                return True
            else:
                response = self.session.get(pdf_info["url"], stream=True, timeout=30)
                response.raise_for_status()
                file_path = folder_path / re.sub(r"[^\w\-_\.]", "_", pdf_info["filename"])
                with open(file_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                print(f"  âœ… å®Œäº†: {file_path.name}")
                return True
        except Exception as e:
            print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False

    def create_readme(self, folder_path, pdf_list, date_str):
        """å®˜å ±ãƒšãƒ¼ã‚¸ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’Readme.mdã¨ã—ã¦è¨˜éŒ²ã—ã¾ã™

        Args:
            folder_path (str): ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¹
            pdf_list (list): pdfã®æƒ…å ±ãƒªã‚¹ãƒˆ
            date_str (str): æ—¥ä»˜ç”¨ æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰
        """
        readme_path = folder_path / f"å®˜å ±_{date_str}.md"
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(f"# å®˜å ± {date_str}\n\n")
            f.write(f"å–å¾—æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if self.test_mode else 'ç„¡åŠ¹'}\n\n")
            f.write("## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
            for pdf in pdf_list:
                f.write(f"- [{pdf['name']}]({pdf['filename']})\n")
                f.write(f"  - URL: {pdf['url']}\n")
        print(f"ğŸ“ READMEä½œæˆå®Œäº†: {readme_path}")

    def run(self):
        """ãƒ¡ã‚¤ãƒ³ã®ãƒ©ãƒ³ãƒŠãƒ¼é–¢æ•°

        Returns:
            bool: æŒ‡å®šã—ãŸæ—¥ä»˜ã§ã€å®˜å ±ã‚’è¦‹ã¤ã‘ã‚‰ã‚ŒãŸã‹ã©ã†ã‹(æ¤œç´¢ã‹ã‚‰90æ—¥ä»¥å†…ãªã‚‰ãƒ’ãƒƒãƒˆã™ã‚‹ã¯ãš)
            list: PDFã®ãƒªãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        """
        print("ğŸš€ å®˜å ±å–å¾—é–‹å§‹")
        all_pdfs = []
        soup = self.fetch_main_page()
        if not soup:
            return False, []

        kanpo_links = self.find_kanpo_link(soup, "pdfDlb")
        if not kanpo_links:
            return False, []

        date_str = self.get_serch_date()
        folder_path = self.create_date_folder(date_str)

        for link_info in kanpo_links:
            print(f"\nğŸ“– å‡¦ç†ä¸­: {link_info['text']}")
            pdf_links = self.fetch_kanpo_page(link_info["url"])
            for pdf_info in pdf_links:
                if self.download_pdf(pdf_info, folder_path):
                    all_pdfs.append(pdf_info)
                time.sleep(1)

        if all_pdfs:
            self.create_readme(folder_path, all_pdfs, date_str)
            return True, all_pdfs
        else:
            return False, []


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å®˜å ±PDFè‡ªå‹•å–å¾—ãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--test", action="store_true", help="ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--date", type=str, help="å¯¾è±¡æ—¥ä»˜ (ä¾‹: 2025-07-03)")
    args = parser.parse_args()

    kanpou_found = False
    pdf_infos = []

    try:
        if args.date:
            try:
                target_date = datetime.strptime(args.date, "%Y-%m-%d")
            except ValueError:
                print("âŒ æ—¥ä»˜ã®å½¢å¼ãŒä¸æ­£ã§ã™ã€‚ä¾‹: 2025-07-03")
                exit(1)
        else:
            target_date = datetime.now()

        fetcher = KanpoFetcher(test_mode=args.test, serch_date=target_date)
        kanpou_found, pdf_infos = fetcher.run()

        if kanpou_found:
            print("ğŸ‰ å®˜å ±å–å¾—æˆåŠŸ")
        else:
            print("âš ï¸ å®˜å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        if len(pdf_infos) > 0:
            print(f"ğŸ“„ PDFãƒªãƒ³ã‚¯æ•°: {len(pdf_infos)}")
            for pdf in pdf_infos:
                print(f"PDFæƒ…å ±:{pdf}")

    except KeyboardInterrupt:
        print("ğŸ‘‹ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼: {e}")

    # Make outputs
    if "GITHUB_OUTPUT" in os.environ:
        with open(os.environ["GITHUB_OUTPUT"], "a") as f:
            print("{0}={1}".format("kanpou_found", kanpou_found), file=f)
            print("{0}={1}".format("pdf_infos", pdf_infos), file=f)
    else:
        print("GITHUB_OUTPUT ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å‡ºåŠ›ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚")
