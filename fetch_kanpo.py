"""
å®˜å ±ã®ãƒšãƒ¼ã‚¸ã‚’è¦‹ã«è¡Œãã€å½“æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚
GitHub Actionã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã€çµæœã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
æ™‚é–“ã¯æ—¥æœ¬æ™‚é–“ï¼ˆAsia/Tokyoï¼‰ã§å–å¾—ã•ã‚Œã¾ã™ã€‚
"""

import os
import re
import time
import argparse
from datetime import datetime
from zoneinfo import ZoneInfo

from pathlib import Path
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup


def str2bool(v):
    """æ–‡å­—åˆ—ã‚’çœŸå½å€¤ã«å¤‰æ›ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    return str(v).lower() in ("true")


class KanpoFetcher:
    def __init__(self, target_date=None):
        """å®˜å ±ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã€PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¯ãƒ©ã‚¹
        Args:
            test_mode (bool, optional): _description_. Defaults to True.
            target_date (datetime, optional): _description_. Defaults to None.
        """
        self.base_url = "https://www.kanpo.go.jp"
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "Mozilla/5.0"})
        self.target_date = target_date or datetime.now(ZoneInfo("Asia/Tokyo"))  # æ—¥æœ¬æ™‚é–“ï¼ˆAsia/Tokyoï¼‰ã§ç¾åœ¨æ™‚åˆ»

    def get_target_date(self):
        """æ¤œç´¢å¯¾è±¡ã®æ—¥ä»˜æ–‡å­—åˆ—(YYYY-MM-DD)ã‚’å–å¾—

        Returns:
            str: æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰
        """

        return self.target_date.strftime("%Y-%m-%d")

    def create_date_folder(self, date_str):
        """æ—¥ä»˜ã«åŸºã¥ã„ã¦ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã™

        Args:
            date_str (str): æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰

        Returns:
            str: ä½œæˆã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
        """
        base_folder = Path("kanpo")
        folder_path = base_folder / date_str
        folder_path.mkdir(parents=True, exist_ok=True)
        return folder_path

    def fetch_main_page(self):
        """å®˜å ±ã®ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¾ã™

        Returns:
            BeautifulSoup: ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        try:
            print("ğŸŒ å®˜å ±ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
            response = self.session.get(self.base_url + "/index.html", timeout=10)
            response.raise_for_status()
            print("âœ… ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ")
            return BeautifulSoup(response.content, "html.parser")
        except Exception as e:
            print(f"âŒ ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {e}")
            return None

    def find_kanpo_link(self, soup, class_filter=None):
        """å®˜å ±ã®ç™ºè¡Œãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ã—ã¾ã™

        Args:
            soup (BeautifulSoup): BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            class_filter (str, optional): æ¤œç´¢ç”¨ã®ã‚¯ãƒ©ã‚¹åãƒ•ã‚£ãƒ«ã‚¿. Defaults to None.

        Returns:
            _type_: _description_
        """

        print("ğŸ” å®˜å ±ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢ä¸­...")

        patterns = [self.target_date.strftime("%Y%m%d")]
        print(f"ğŸ“… æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: {patterns}")
        print(f"ğŸ¯ ä½¿ç”¨ã™ã‚‹ã‚¯ãƒ©ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {class_filter}")

        links = []
        link_candidates = (
            soup.find_all("a", href=True, class_=class_filter) if class_filter else soup.find_all("a", href=True)
        )

        for link in link_candidates:
            link_text = link.get_text(strip=True)
            href = link.get("href", "")
            for pattern in patterns:
                if pattern in link_text or pattern in href:
                    full_url = urljoin(self.base_url, href)
                    links.append({"url": full_url, "text": link_text, "pattern_matched": pattern})
                    print(f"  âœ… ç™ºè¦‹: {link_text} (ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern})")
                    break

        if not links:
            print("âš ï¸ å®˜å ±ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            for i, link in enumerate(link_candidates[:10]):
                text = link.get_text(strip=True)
                if text and len(text) > 3:
                    print(f"    {i + 1}. {text}")
        return links

    def fetch_kanpo_page(self, kanpo_url):
        """å®˜å ±ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã€PDFãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã™

        Args:
            kanpo_url (str): å®˜å ±ã®URL

        Returns:
            list: å®˜å ±ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã—ãŸPDFãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """

        print(f"ğŸ“– å®˜å ±ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ä¸­: {kanpo_url}")

        try:
            response = self.session.get(kanpo_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            pdf_links = []
            date_text = None

            date_element = soup.find("p", class_="date")
            if date_element:
                date_text = date_element.get_text(strip=True)
                print(f"ğŸ“… å®˜å ±ã®æ—¥ä»˜: {date_text}")
            else:
                print("âš ï¸ p ã« date ã‚¯ãƒ©ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“")

            for iframe in soup.find_all("iframe", src=True):
                src = iframe["src"]
                if src.lower().endswith(".pdf"):
                    pdf_url = urljoin(kanpo_url, src)
                    pdf_name = os.path.basename(urlparse(src).path)
                    pdf_links.append({"url": pdf_url, "name": date_text, "filename": pdf_name, "source": "iframe"})
                    print(f"  ğŸ“„ PDFç™ºè¦‹ (<iframe>): {pdf_name}")

            print(f"ğŸ“Š PDFæ•°: {len(pdf_links)}")
            return pdf_links
        except Exception as e:
            print(f"âŒ å®˜å ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {e}")
            return []

    def download_pdf(self, pdf_info, folder_path):
        """å®˜å ±ã®PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™

        Args:
            pdf_info (dict): pdfã®æƒ…å ±ï¼ˆURLã€åå‰ã€ãƒ•ã‚¡ã‚¤ãƒ«åãªã©ï¼‰
            folder_path (str): ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹

        Returns:
            bool: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹
        """

        try:
            print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {pdf_info['name']}")
            response = self.session.get(pdf_info["url"], stream=True, timeout=30)
            response.raise_for_status()
            file_path = folder_path / re.sub(r"[^\w\-_\.]", "_", pdf_info["filename"])
            with open(file_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            print(f"  âœ… å®Œäº†: {file_path.name}")
            return True
        except Exception as e:
            print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False

    def create_readme(self, folder_path, pdf_list, date_str):
        """å®˜å ±ãƒšãƒ¼ã‚¸ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’Readme.mdã¨ã—ã¦è¨˜éŒ²ã—ã¾ã™

        Args:
            folder_path (str): ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¹
            pdf_list (list): pdfã®æƒ…å ±ãƒªã‚¹ãƒˆ
            date_str (str): æ—¥ä»˜ç”¨ æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰
        """
        readme_path = folder_path / f"å®˜å ±_{date_str}.md"
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(f"# å®˜å ± {date_str}\n\n")
            f.write(f"å–å¾—æ—¥æ™‚: {datetime.now(ZoneInfo('Asia/Tokyo')).strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
            for pdf in pdf_list:
                f.write(f"- [{pdf['name']}]({pdf['filename']})\n")
                f.write(f"  - URL: {pdf['url']}\n")
        print(f"ğŸ“ READMEä½œæˆå®Œäº†: {readme_path}")

    def run(self):
        """ãƒ¡ã‚¤ãƒ³ã®ãƒ©ãƒ³ãƒŠãƒ¼é–¢æ•°

        Returns:
            bool: æŒ‡å®šã—ãŸæ—¥ä»˜ã§ã€å®˜å ±ã‚’è¦‹ã¤ã‘ã‚‰ã‚ŒãŸã‹ã©ã†ã‹(æ¤œç´¢ã‹ã‚‰90æ—¥ä»¥å†…ãªã‚‰ãƒ’ãƒƒãƒˆã™ã‚‹ã¯ãš)
            list: PDFã®ãƒªãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        """
        print("ğŸš€ å®˜å ±å–å¾—é–‹å§‹")
        all_pdfs = []
        soup = self.fetch_main_page()
        if not soup:
            return False, []

        kanpo_links = self.find_kanpo_link(soup, "pdfDlb")
        if not kanpo_links:
            return False, []

        date_str = self.get_target_date()
        folder_path = self.create_date_folder(date_str)

        for link_info in kanpo_links:
            print(f"\nğŸ“– å‡¦ç†ä¸­: {link_info['text']}")
            pdf_links = self.fetch_kanpo_page(link_info["url"])
            for pdf_info in pdf_links:
                if self.download_pdf(pdf_info, folder_path):
                    all_pdfs.append(pdf_info)
                time.sleep(1)

        if all_pdfs:
            self.create_readme(folder_path, all_pdfs, date_str)
        else:
            return False, []

        return True, all_pdfs

    def test_run(self):
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ©ãƒ³ãƒŠãƒ¼é–¢æ•°(ãƒšãƒ¼ã‚¸ã‚’ãƒ•ã‚§ãƒƒãƒã—ã¦ã€pdfã®ãƒªãƒ³ã‚¯ã ã‘ã‚’å–å¾—ã™ã‚‹)

        Returns:
            bool: æŒ‡å®šã—ãŸæ—¥ä»˜ã§ã€å®˜å ±ã‚’è¦‹ã¤ã‘ã‚‰ã‚ŒãŸã‹ã©ã†ã‹(æ¤œç´¢ã‹ã‚‰90æ—¥ä»¥å†…ãªã‚‰ãƒ’ãƒƒãƒˆã™ã‚‹ã¯ãš)
            list: PDFã®ãƒªãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        """
        print("ğŸš€ å®˜å ±ãƒªãƒ³ã‚¯å–å¾—é–‹å§‹")
        all_pdfs = []
        soup = self.fetch_main_page()
        if not soup:
            return False, all_pdfs

        kanpo_links = self.find_kanpo_link(soup, "pdfDlb")
        if not kanpo_links:
            print("âš ï¸ ç™ºè¡Œã•ã‚ŒãŸå®˜å ±ã®URL(ã‚¯ãƒ©ã‚¹å:pdfDlb)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return False, all_pdfs

        for link_info in kanpo_links:
            print(f"\nğŸ“– å‡¦ç†ä¸­: {link_info['text']}")
            pdf_links = self.fetch_kanpo_page(link_info["url"])
            for pdf_info in pdf_links:
                all_pdfs.append(pdf_info)
                time.sleep(1)

        if not all_pdfs:
            print("âš ï¸ PDFãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return False, all_pdfs

        return True, all_pdfs


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å®˜å ±PDFè‡ªå‹•å–å¾—ãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--dlpdf", type=str2bool, nargs="?", const=True, default=False, help="PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹")
    parser.add_argument("--date", type=str, help="å¯¾è±¡æ—¥ä»˜ (ä¾‹: 2025-07-03)")
    args = parser.parse_args()

    kanpou_found = False
    pdf_infos = []

    try:
        if args.date:
            try:
                target_date = datetime.strptime(args.date, "%Y-%m-%d")
            except ValueError:
                print("âŒ æ—¥ä»˜ã®å½¢å¼ãŒä¸æ­£ã§ã™ã€‚ä¾‹: YYYY-MM-DD")
                print(f"å…¥åŠ›ã•ã‚ŒãŸæ—¥ä»˜: {args.date}")
                exit(1)
        else:
            target_date = datetime.now(ZoneInfo("Asia/Tokyo"))

        fetcher = KanpoFetcher(target_date=target_date)

        if args.dlpdf:
            print("ğŸš€ æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰: å®˜å ±PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
            kanpou_found, pdf_infos = fetcher.run()
        else:
            print("ğŸ” ç¢ºèªãƒ¢ãƒ¼ãƒ‰: å®˜å ±ãƒªãƒ³ã‚¯ã®ã¿ã‚’å–å¾—ã—ã¾ã™")
            kanpou_found, pdf_infos = fetcher.test_run()

        if kanpou_found:
            print(f"ğŸ“„ kanpou_found: {kanpou_found}")
            print("ğŸ‰ å®˜å ±å–å¾—æˆåŠŸ")
        else:
            print("âš ï¸ å®˜å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        if len(pdf_infos) > 0:
            print(f"ğŸ“„ PDFãƒªãƒ³ã‚¯æ•°: {len(pdf_infos)}")
            for pdf in pdf_infos:
                print(f"PDFæƒ…å ±:{pdf}")

    except KeyboardInterrupt:
        print("ğŸ‘‹ ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼: {e}")

    # Make outputs
    if "GITHUB_OUTPUT" in os.environ:
        output_path = os.environ.get("GITHUB_OUTPUT")
        print("GITHUB_OUTPUT ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚å‡ºåŠ›ã‚’è¡Œã„ã¾ã™ã€‚")
        with open(output_path, "a") as fh:
            print(f"kanpou_found={kanpou_found}", file=fh)
            print(f"pdf_infos={pdf_infos}", file=fh)

        if output_path:
            print(f"GITHUB_OUTPUT ã®ãƒ‘ã‚¹: {output_path}")
            try:
                with open(output_path, "r") as f:
                    content = f.read()
                print("GITHUB_OUTPUT ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«:")
                print(content)
            except Exception as e:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            print("GITHUB_OUTPUT ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    else:
        print("GITHUB_OUTPUT ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å‡ºåŠ›ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚")
